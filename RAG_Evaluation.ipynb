{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriver Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@3: 0.67\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K for a single query.\n",
    "\n",
    "    Parameters:\n",
    "    - retrieved_docs: List of document IDs retrieved by the retriever.\n",
    "    - relevant_docs: List of document IDs that are relevant to the query.\n",
    "    - k: Top K documents to consider.\n",
    "\n",
    "    Returns:\n",
    "    - Precision@K score.\n",
    "    \"\"\"\n",
    "    top_k_docs = retrieved_docs[:k]  # Take the top K retrieved documents\n",
    "    relevant_retrieved = len(set(top_k_docs) & set(relevant_docs))  # Relevant docs in top K\n",
    "    precision = relevant_retrieved / k if k > 0 else 0  # Calculate precision\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "# Example usage\n",
    "retrieved_docs = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"]\n",
    "relevant_docs = [\"doc1\", \"doc2\", \"doc9\"]\n",
    "k = 3\n",
    "\n",
    "precision_k = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "print(f\"Precision@{k}: {precision_k:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.04\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference_text, generated_text):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for generated text compared to a reference.\n",
    "\n",
    "    Parameters:\n",
    "    - reference_text: List of words in the reference text.\n",
    "    - generated_text: List of words in the generated text.\n",
    "\n",
    "    Returns:\n",
    "    - BLEU score.\n",
    "    \"\"\"\n",
    "    smooth = SmoothingFunction().method1  # Smoothing to handle 0 counts\n",
    "    score = sentence_bleu([reference_text], generated_text, smoothing_function=smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Example usage\n",
    "reference_text = \"The sun rose over the mountains\".split()\n",
    "generated_text = \"The sunrise lit up the mountain peaks.\".split()\n",
    "\n",
    "bleu_score = calculate_bleu(reference_text, generated_text)\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.18 seconds, 5.56 sentences/sec\n",
      "Precision: 0.9396982192993164\n",
      "Recall: 0.9341763854026794\n",
      "F1 Score: 0.9369291067123413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Example texts\n",
    "candidates = [\"The sun rose over the mountains\"]\n",
    "references = [\"The sunrise lit up the mountain peaks.\"]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(candidates, references, lang='en', verbose=True)\n",
    "\n",
    "# Print scores\n",
    "print(f\"Precision: {P.mean()}\")\n",
    "print(f\"Recall: {R.mean()}\")\n",
    "print(f\"F1 Score: {F1.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
